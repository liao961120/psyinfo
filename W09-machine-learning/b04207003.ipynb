{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psychoinformatics - Week 9 (Exercises)\n",
    "by 廖永賦 (b04207003@ntu.edu.tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 檢查 machine learning pipeline (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 請打亂原本的Y觀察正確率是否和chance level (0.33)有差異? 若有, why? (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 0 1 0 1 2 0 1 2 1 0 0 1 1 2 1 0 2 2 1 0 1 1 1 2 2 2 0 2 0 1 0 2 1 0\n",
      " 1 2 2 1 2 2 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 2 1 2 2 2 1 0 1 2 2 2 1 0 2 0\n",
      " 2 2 1 0 2 1 2 1 2 1 2 0 1 0 2 1 2 2 1 2 2 0 0 0 1 0 1 1 0 0 2 0 0 0 2 2 0\n",
      " 2 0 1 2 1 1 2 0 0 0 2 0 2 0 1 1 2 2 0 0 1 1 1 1 0 0 2 2 2 2 1 2 1 1 0 0 1\n",
      " 0 1]\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "# 本題在研究打亂X和打亂Y有差別嗎?\n",
    "import sklearn\n",
    "from random import shuffle\n",
    "iris = datasets.load_iris()\n",
    "X=iris.data\n",
    "Y=iris.target\n",
    "shuffle(Y)   # 若在這裡 shuffle，則正確率遠高於 0.33\n",
    "print(Y)\n",
    "clf=neighbors.KNeighborsClassifier(1)\n",
    "clf.fit(X,Y)\n",
    "#shuffle(Y)  # 若在這裡 shuffle，則正確率接近 0.33\n",
    "accuracy=np.mean(clf.predict(X)==Y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `shuffle(Y)` 在 `clf.fit(X,Y)` 之前\n",
    "此情況下，**正確率非常高，接近 1**。  \n",
    "原因是因為 **training dataset 和 testing dataset 是相同的**，  \n",
    "所以這個 classifier 事實上是在判斷自己已經「看過」的資料，才會有如此高的正確率。\n",
    "\n",
    "也因此，若將 `shuffle(Y)` 置於 `clf.fit(X,Y)` 之後，  \n",
    "正確率就會變得相當接近 0.33。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 請用統計檢定accuracies中的結果是否和chance level (0.5)有差異? 若有, why? (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "Y=np.remainder(range(200),2) \n",
    "print(Y) #Y的0和1個數一樣多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跑一百次測試:\n",
    "clf=svm.SVC()\n",
    "accuracies=[]\n",
    "for i in range(100): \n",
    " X=np.random.rand(200,2) # X取亂數\n",
    " kf=cross_validation.KFold(len(Y),len(Y),shuffle=True) # Leave-one-out cross-validation\n",
    " sc=cross_validation.cross_val_score(clf,X,Y,cv=kf)\n",
    " accuracies.append(sc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.515, 0.445, 0.505, 0.51, 0.475, 0.425, 0.455, 0.475, 0.55, 0.455, 0.515, 0.0, 0.5, 0.53, 0.485, 0.535, 0.49, 0.05, 0.0, 0.475, 0.5, 0.565, 0.565, 0.47, 0.02, 0.51, 0.53, 0.48, 0.485, 0.485, 0.53, 0.475, 0.295, 0.53, 0.0, 0.52, 0.44, 0.455, 0.47, 0.51, 0.54, 0.0, 0.47, 0.445, 0.5, 0.45, 0.45, 0.455, 0.54, 0.5, 0.5, 0.435, 0.085, 0.52, 0.47, 0.0, 0.51, 0.53, 0.015, 0.48, 0.51, 0.49, 0.455, 0.32, 0.395, 0.42, 0.52, 0.385, 0.535, 0.515, 0.53, 0.505, 0.51, 0.48, 0.46, 0.005, 0.26, 0.56, 0.545, 0.53, 0.515, 0.52, 0.55, 0.475, 0.0, 0.48, 0.0, 0.47, 0.415, 0.575, 0.58, 0.0, 0.0, 0.41, 0.415, 0.45, 0.44, 0.485, 0.595]\n",
      "0.4192499999999999\n",
      "-4.657073716901971\n",
      "9.98697061261267e-06\n"
     ]
    }
   ],
   "source": [
    "# Please do your statistical tests here:\n",
    "print(accuracies)\n",
    "import scipy\n",
    "ttest = scipy.stats.ttest_1samp(accuracies, 0.5)\n",
    "print(sum(accuracies)/len(accuracies))\n",
    "print(ttest.statistic)\n",
    "print(ttest.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有差異，因為 Leave-one-out cross-validation 會取出一個(假設是 1)，此時，剩下的 data 中 0 就會比較多，因此 classifier 會傾向預測被拿出去的 data 是 0 (但事實上是 1)，因而造成正確率下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please submit this file to http://hpc.psy.ntu.edu.tw/info before noon next Wednesday."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
